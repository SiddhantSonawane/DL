{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50d92c04-a315-451e-9b27-9087dd7dce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac582f40-ac43-4028-a859-c6b64eac70f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context-Target Pairs: [(array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32)), (array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "       0.], dtype=float32)), (array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0.], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0.], dtype=float32)), (array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32), array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32)), (array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.], dtype=float32))]\n"
     ]
    }
   ],
   "source": [
    "# Generate training data using skip-gram model (for pairs)\n",
    "window_size = 2  # Context window size\n",
    "\n",
    "pairs, _ = skipgrams(\n",
    "    tokenizer.texts_to_sequences([text])[0],\n",
    "    vocabulary_size=vocab_size,\n",
    "    window_size=window_size,\n",
    "    negative_samples=0\n",
    ")\n",
    "\n",
    "# Prepare input and output for CBOW\n",
    "def generate_cbows(pairs, vocab_size):\n",
    "    X = []\n",
    "    y = []\n",
    "    for pair in pairs:\n",
    "        context_word, target_word = pair\n",
    "        context_one_hot = tf.keras.utils.to_categorical(context_word, vocab_size)\n",
    "        target_one_hot = tf.keras.utils.to_categorical(target_word, vocab_size)\n",
    "        X.append(context_one_hot)\n",
    "        y.append(target_one_hot)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = generate_cbows(pairs, vocab_size)\n",
    "print(\"Context-Target Pairs:\", list(zip(X, y))[:5])  # Display sample pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525fb19f-e179-4c41-b129-601fe4127720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2/2 [==============================] - 2s 16ms/step - loss: 2.8748 - accuracy: 0.0806\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 2.8721 - accuracy: 0.0806\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.8698 - accuracy: 0.0968\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.8674 - accuracy: 0.0968\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.8653 - accuracy: 0.1129\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.8631 - accuracy: 0.1129\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.8610 - accuracy: 0.1129\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.8588 - accuracy: 0.1129\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.8568 - accuracy: 0.1129\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.8546 - accuracy: 0.1129\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.8526 - accuracy: 0.1129\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 2.8505 - accuracy: 0.1129\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 2.8484 - accuracy: 0.1129\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.8463 - accuracy: 0.1129\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 2.8443 - accuracy: 0.1129\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.8423 - accuracy: 0.1129\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.8402 - accuracy: 0.1129\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.8382 - accuracy: 0.1129\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.8361 - accuracy: 0.1129\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.8340 - accuracy: 0.1129\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.8321 - accuracy: 0.1129\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 2.8299 - accuracy: 0.1129\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.8280 - accuracy: 0.1129\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.8259 - accuracy: 0.1129\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.8239 - accuracy: 0.1129\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.8219 - accuracy: 0.1129\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 2.8198 - accuracy: 0.1129\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.8177 - accuracy: 0.1290\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.8157 - accuracy: 0.1290\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 2.8137 - accuracy: 0.1290\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 2.8117 - accuracy: 0.1290\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 0s/step - loss: 2.8097 - accuracy: 0.1290\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.8075 - accuracy: 0.1290\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 0s/step - loss: 2.8055 - accuracy: 0.1290\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 2.8036 - accuracy: 0.1290\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 2.8014 - accuracy: 0.1290\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.7994 - accuracy: 0.1290\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 2.7974 - accuracy: 0.1290\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.7952 - accuracy: 0.1290\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 2.7932 - accuracy: 0.1290\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 0s/step - loss: 2.7911 - accuracy: 0.1290\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 2.7891 - accuracy: 0.1290\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.7869 - accuracy: 0.1290\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 2.7849 - accuracy: 0.1290\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.7827 - accuracy: 0.1290\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.7807 - accuracy: 0.1290\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.7785 - accuracy: 0.1290\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 2.7764 - accuracy: 0.1290\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 2.7742 - accuracy: 0.1290\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 0s/step - loss: 2.7721 - accuracy: 0.1290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x239a3a5ba60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten\n",
    "\n",
    "# Define CBOW model architecture\n",
    "cbow_model = Sequential([\n",
    "    Dense(10, input_shape=(vocab_size,), activation='relu'),  # Hidden layer\n",
    "    Dense(vocab_size, activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "cbow_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CBOW model\n",
    "cbow_model.fit(X, y, epochs=50, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5087a85d-af1e-4254-a5f9-ea03f19779af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: machine, Embedding: [ 0.30499768 -0.0234406   0.20189868  0.525097    0.53276277  0.3227995\n",
      "  0.00905425 -0.13517871 -0.33666033 -0.44810674]\n",
      "Word: learning, Embedding: [-0.11615771 -0.36203852 -0.28251657  0.09548775 -0.06441048 -0.37272066\n",
      "  0.4659277   0.12067108 -0.37897158 -0.09174126]\n",
      "Word: provides, Embedding: [-0.42041713 -0.23929821  0.4051469  -0.1924701  -0.38650873 -0.10862246\n",
      "  0.44456312 -0.21550444  0.23427449  0.12469187]\n",
      "Word: systems, Embedding: [ 0.00276678 -0.05291107  0.15985373 -0.2647965  -0.04449818 -0.25657672\n",
      "  0.08335418  0.00155183  0.25679064 -0.15695485]\n",
      "Word: the, Embedding: [-0.1007784   0.36344302 -0.15892911  0.03579071  0.3712883  -0.3389543\n",
      " -0.2066865   0.3527219   0.25335526  0.1909682 ]\n",
      "Word: ability, Embedding: [ 0.29782155  0.42543942 -0.11717078 -0.09540278 -0.29750288  0.3803732\n",
      "  0.21087201 -0.11057386 -0.46104696 -0.17724031]\n",
      "Word: to, Embedding: [-0.23966275  0.28592697 -0.00273817 -0.21239585 -0.38467112  0.26177365\n",
      " -0.36579296  0.01985047 -0.23842598  0.42000747]\n",
      "Word: automatically, Embedding: [-0.1093502   0.38892534  0.12155177  0.12203749  0.38341692 -0.374395\n",
      " -0.14151517 -0.07127371  0.22565843  0.38559064]\n",
      "Word: learn, Embedding: [ 0.32389453 -0.21648173 -0.34978995 -0.23651543  0.02430798  0.4040111\n",
      "  0.2879899  -0.11513263 -0.279869    0.35124046]\n",
      "Word: and, Embedding: [ 0.01540299 -0.33278614  0.4057763  -0.15268147 -0.04597476  0.4959585\n",
      "  0.10101551  0.22619715 -0.3988905   0.18579826]\n",
      "Word: improve, Embedding: [ 0.1298573  -0.42275336  0.48439866  0.41938156  0.12693729  0.34222096\n",
      " -0.02765856  0.02126216 -0.39374027 -0.32543883]\n",
      "Word: from, Embedding: [-0.25335094 -0.16508082  0.31777623  0.02932052  0.3135349   0.21186766\n",
      " -0.43181598  0.27664942 -0.39298755 -0.15762094]\n",
      "Word: experience, Embedding: [ 0.3860102   0.5036299   0.04727805 -0.17103198 -0.3105312  -0.21008953\n",
      " -0.3791782  -0.18891752 -0.37653613  0.07756667]\n",
      "Word: without, Embedding: [ 0.15222473  0.34233665 -0.2767412   0.38552675  0.1492906   0.30297643\n",
      " -0.05560565  0.47320327  0.15258095  0.49415562]\n",
      "Word: being, Embedding: [ 0.19489256 -0.32646987 -0.41927263  0.38941824  0.36345991  0.24242139\n",
      "  0.07307004 -0.24282861 -0.22338769 -0.20655394]\n",
      "Word: explicitly, Embedding: [-0.4044267  -0.2123782   0.08118875  0.4338067   0.28678083 -0.10671026\n",
      "  0.05190433  0.02299336  0.05645645 -0.21794541]\n",
      "Word: programmed, Embedding: [-0.15602228  0.47717115 -0.07855093  0.27137867 -0.26801884 -0.1197719\n",
      " -0.04081805 -0.27733618 -0.1313777   0.4227318 ]\n"
     ]
    }
   ],
   "source": [
    "# Extract the embeddings for each word\n",
    "embeddings = cbow_model.layers[0].get_weights()[0]\n",
    "\n",
    "# Display a few word embeddings\n",
    "for word, idx in word_to_index.items():\n",
    "    print(f\"Word: {word}, Embedding: {embeddings[idx]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07b6720d-15be-40da-8018-cc4ac415fbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 62ms/step\n",
      "Context word: 'machine' -> Predicted target word: 'provides'\n"
     ]
    }
   ],
   "source": [
    "# Function to predict target word based on context\n",
    "def predict_target(context_word):\n",
    "    context_index = word_to_index[context_word]\n",
    "    context_one_hot = tf.keras.utils.to_categorical([context_index], vocab_size)\n",
    "    prediction = cbow_model.predict(context_one_hot)\n",
    "    predicted_index = np.argmax(prediction)\n",
    "    predicted_word = index_to_word[predicted_index]\n",
    "    return predicted_word\n",
    "\n",
    "# Test the CBOW model with a sample context word\n",
    "sample_context = \"machine\"\n",
    "predicted_word = predict_target(sample_context)\n",
    "print(f\"Context word: '{sample_context}' -> Predicted target word: '{predicted_word}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f402f2d3-f9e4-4eee-9cd8-53c0b834c3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
